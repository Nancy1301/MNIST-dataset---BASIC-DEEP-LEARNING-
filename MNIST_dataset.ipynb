{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IXPDpbsr9hW"
      },
      "source": [
        "# **MNIST dataset**\r\n",
        "\r\n",
        "The MNIST dataset refers to handwritten digit recognition.\r\n",
        "\r\n",
        "It is considered to be the most common and the 'Hello World' problem of deep learning because:\r\n",
        "\r\n",
        "1. It is a very visual problem\r\n",
        "2. Extremely common\r\n",
        "3. Easy to build up to CNN\r\n",
        "4. Very big and Preprocessed\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RFeuZaqs5bv"
      },
      "source": [
        "# **About the datset**\r\n",
        "\r\n",
        "The dataset provides 70,000 images Each of 28x28 pixels,overall having 784 pixels of handwritten digits. It can be thought of as a 28x28 matrix where input values are from 0 to 255.\r\n",
        "\r\n",
        "On the greysacle:\r\n",
        "\r\n",
        "0 --> purely black\r\n",
        "\r\n",
        "255 --> purely white\r\n",
        "\r\n",
        "# **About the Approach**\r\n",
        "The approach for a deep neral network is to \"flatten\" each image into a vector of 284x1. The goal is to write an algorithm to detect which digit is written. Since there is only 10 digits, it is a classification problem with 10 classes\r\n",
        "\r\n",
        "Our model will have 2 hidden layers as for a huge dataset like this, 2 hidden layers are enough to provide with high accuracy rate.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y-BJKfsu4GF"
      },
      "source": [
        "# **Steps carried out:**\r\n",
        "1. Preparing our data and preprocessing it. Creating training,validation and test datasets.\r\n",
        "2. Outline our model and choose activation functions.\r\n",
        "3. Set the appropriate and advanced customizers and the loss functions.\r\n",
        "4. Make the algorithm learn through backpropagation techniques, and at each epoch we will validate it.\r\n",
        "5. Finally, we will test the accuracy of our model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSEH4TZrpIp"
      },
      "source": [
        "#Importing relevant libraries\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaTssITmw3iY"
      },
      "source": [
        "#loading the data\r\n",
        "mnist_dataset, mnist_info = tfds.load(name = 'mnist',with_info=True, as_supervised=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhPOmnWYy_tg"
      },
      "source": [
        "# Creating training,validation and test datasets.\r\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\r\n",
        "# By default MNIST of Tensorflows only contains train and test data, \r\n",
        "# therefore we will have to extract validation dataset from the train data(as train data is more as compared to test data.\r\n",
        "# train data --> 60,000\r\n",
        "# test data --> 10,000\r\n",
        "# Lets take 10% of train data as validation data.\r\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\r\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\r\n",
        "\r\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\r\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\r\n",
        "\r\n",
        "# Scaling the data between 0 to 1.\r\n",
        "def scale(image,label):\r\n",
        "  image = tf.cast(image, tf.float32)\r\n",
        "  image /=255\r\n",
        "  return image,label\r\n",
        "\r\n",
        "# This will scale all the data\r\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\r\n",
        "scaled_test_data = mnist_test.map(scale)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLfvnrAe1-Qz",
        "outputId": "63e27e39-f5b8-4593-b867-601c822d9291"
      },
      "source": [
        "# Now we will shuffle(keeping th same info but in different order)the data and create validation sets\r\n",
        "# While dealing with enormous dataset, we cant shuffle all data at once, hence we create buffer, taking 10,000 at a time\r\n",
        "\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\r\n",
        "\r\n",
        "shuffled_train_and_validation_data "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUK6gQpu5MNR"
      },
      "source": [
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\r\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4YGNwc5s29"
      },
      "source": [
        "# Performing batching\r\n",
        "# batch size = 1 --> SGD\r\n",
        "# batch size = no. of samples = GD\r\n",
        "# therefore, we want\r\n",
        "# 1< batch size < no. of samples = mini-batch GD.\r\n",
        "batch_size = 100\r\n",
        "train_data = train_data.batch(batch_size)\r\n",
        "train_data\r\n",
        "test_data = mnist_test.map(scale)\r\n",
        "test_data = test_data.batch(num_test_samples)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "937YrnKz6bbB"
      },
      "source": [
        "# when batching, we generally find average loss and accuracy\r\n",
        "validation_data = validation_data.batch(num_validation_samples) # in this way, we'll create a new column in our tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_aruFh7A-P"
      },
      "source": [
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D89JzHN8NgT"
      },
      "source": [
        "# Outlining the model\r\n",
        "Width and depth are the hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8j_bXzg7xdr",
        "outputId": "32369ba7-b9dd-4d03-baf5-30a876cb08d1"
      },
      "source": [
        "# Declaring 3 variables for width,inputs,outputs\r\n",
        "hidden_layer_size = 100 # as taking suboptimal \r\n",
        "input_size = 784\r\n",
        "output_size = 10 # as we have 10 digits\r\n",
        "\r\n",
        "# 1st layer ->input layer\r\n",
        "# Because we dont know CNN rightnow, we will have to flatten the images using flatten function\r\n",
        "model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Flatten(input_shape = (28,28,1)),\r\n",
        "                             # this will be finding the dot product of inputs and weights and adding the bias\r\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \r\n",
        "                             # Until now we got our first hidden layer  and our flatten inputs\r\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # creating second hidden layer\r\n",
        "                             tf.keras.layers.Dense(output_size, activation='softmax')\r\n",
        "                             # as for classifying we should opt for softmax\r\n",
        "\r\n",
        "])\r\n",
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f63ba12e650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTvCmaabBX2e"
      },
      "source": [
        "# Choosing the optimizer and loss functions\r\n",
        "\r\n",
        "The three types of loss in tensorflow :\r\n",
        " 1. Binary crossentropy for binary dataset\r\n",
        " 2. Categorical crossenetropy : \r\n",
        " \r\n",
        " This expects that you have already hot-encoded your targets\r\n",
        " 3. Sparse_categorical crossentropy:\r\n",
        "\r\n",
        " Almost similar, but it will apply hot-encoding\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfvt69Dc-9Kh"
      },
      "source": [
        "model.compile(optimizer='adam', loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOhoTOyxCUBl"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU5r7hd8CTCx",
        "outputId": "a732338a-a025-4e85-cc4e-a7c1f585d44d"
      },
      "source": [
        "NUM_EPOCHS = 7\r\n",
        " # arbitrarily set\r\n",
        "\r\n",
        "# now fit the model\r\n",
        "# When we reach the maximum number of epochs, training will be over.\r\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "540/540 - 5s - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.0314 - val_accuracy: 0.9900\n",
            "Epoch 2/7\n",
            "540/540 - 5s - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
            "Epoch 3/7\n",
            "540/540 - 5s - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.0182 - val_accuracy: 0.9933\n",
            "Epoch 4/7\n",
            "540/540 - 4s - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.0183 - val_accuracy: 0.9935\n",
            "Epoch 5/7\n",
            "540/540 - 4s - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0135 - val_accuracy: 0.9947\n",
            "Epoch 6/7\n",
            "540/540 - 4s - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
            "Epoch 7/7\n",
            "540/540 - 4s - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0132 - val_accuracy: 0.9957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63b9eb3d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd2DPaVxLDxN"
      },
      "source": [
        "We got the highest validation accuracy so far after changing the number of epochs and hidden layer size.\r\n",
        "The accuracy we obtained is the accuracy of the algorithm.\r\n",
        "We have overfitted our validation dataset.\r\n",
        "\r\n",
        "Our validation accuracy comes out to be -> 99.6%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLslxMHtLVPE"
      },
      "source": [
        "# **Testing the data**\r\n",
        "\r\n",
        "Test data makes sure that our hyperparameters - width,depth,batchsize, no. of epochs, etc dont overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-RMNAybKgG0",
        "outputId": "4654e57b-938f-4631-fc7a-f41a5f14b039"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\r\n",
        "print ('Test Loss: {0:.2f}. Test Accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.0908 - accuracy: 0.9788\n",
            "Test Loss: 0.09. Test Accuracy: 97.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZSKifg1Np5H"
      },
      "source": [
        "**Therefore, we come to an end of training our model with 97.9% accuracy**"
      ]
    }
  ]
}